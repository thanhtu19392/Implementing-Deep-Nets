{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Alexnet\n",
    "\n",
    "For reference: [Alexnet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "\n",
    "Alexnet is a convolutional neural network which compteted in the **ImageNet Large Scale Visual Recognition Challenge** in 2012. For those that aren’t familiar, this competition can be thought of as the annual Olympics of computer vision, where teams from across the world compete to see who has the best computer vision model for tasks such as classification, localization, detection, and more. The network achieved a top-5 error of 15.3% (Top 5 error is the rate at which, given an image, the model does not output the correct label with its top 5 predictions), more than 10.8 percentage points ahead of the runner up.     \n",
    "AlexNet was designed by the SuperVision group, consisting of Alex Krizhevsky, Geoffrey Hinton, and Ilya Sutskever. \n",
    "\n",
    "### Structure of the AlexNet: \n",
    "The network was made up of 5 conv layers, max-pooling layers, dropout layers, and 3 fully connected layers. The network they designed was used for classification with 1000 possible categories.\n",
    "\n",
    "Input -> Conv1 -> Pool1 -> Conv2 -> Pool2 -> Conv3 -> Conv4 -> Pool4 -> Conv5 -> Pool5 -> FC1 -> FC2 -> FC3 -> output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://www.researchgate.net/profile/Walid_Aly/publication/312188377/figure/fig4/AS:448996423540740@1484060497977/Figure-7-An-illustration-of-the-architecture-of-AlexNet-CNN-14.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details: \n",
    "Layer 1:   \n",
    "Input Image size is – 224 x 224 x 3    \n",
    "Number of filters – 96    \n",
    "Filter size – 11 x 11 x 3    \n",
    "Stride – 4    \n",
    "Layer 1 Output    \n",
    "224/4 x 224/4 x 96 = 55 x 55 x 96 (because of stride 4)   \n",
    "Split across 2 GPUs – So 55 x 55 x 48 for each GPU   \n",
    "\n",
    "Layer 2 is a Max Pooling Followed by Convolution     \n",
    "Input – 55 x 55 x 96     \n",
    "Max pooling – 55/2 x 55/2 x 96 = 27 x 27 x 96     \n",
    "Number of filters – 256     \n",
    "Filter size – 5 x 5 x 48     \n",
    "Layer 2 Output     \n",
    "27 x 27 x 256      \n",
    "Split across 2 GPUs – So 27 x 27 x 128 for each GPU     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def alex_preprocess(x):\n",
    "    return x\n",
    "\n",
    "class Alexnet():\n",
    "    \"\"\"The VGG 16 Imagenet model\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.FILE_PATH = 'http://files.fast.ai/models/'\n",
    "        self.WEIGHTS_PATH = \"datasets/\"\n",
    "\n",
    "        self.create()\n",
    "#         self.get_classes()\n",
    "\n",
    "\n",
    "#     def get_classes(self):\n",
    "#        fname = 'imagenet_class_index.json'\n",
    "#        fpath = get_file(fname, self.FILE_PATH+fname, cache_subdir='models')\n",
    "#        with open(fpath) as f:\n",
    "#            class_dict = json.load(f)\n",
    "#        self.classes = [class_dict[str(i)][1] for i in range(len(class_dict))]\n",
    "\n",
    "    def predict(self, imgs, details=False):\n",
    "        all_preds = self.model.predict(imgs)\n",
    "        idxs = np.argmax(all_preds, axis=1)\n",
    "        preds = [all_preds[i, idxs[i]] for i in range(len(idxs))]\n",
    "        classes = [self.classes[idx] for idx in idxs]\n",
    "        return np.array(preds), idxs, classes\n",
    "\n",
    "\n",
    "    def ConvBlock(self, layers, filters, nb_rowcol=3):\n",
    "        model = self.model\n",
    "        for i in range(layers):\n",
    "            model.add(ZeroPadding2D((1, 1)))\n",
    "            model.add(Conv2D(filters, (nb_rowcol, nb_rowcol), activation='relu',data_format='channels_first'))\n",
    "        model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "\n",
    "\n",
    "    def FCBlock(self):\n",
    "        model = self.model\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    def create(self):\n",
    "        model = self.model = Sequential()\n",
    "        model.add(Lambda(alex_preprocess, input_shape=(3,227,227)))\n",
    "        \n",
    "        self.ConvBlock(1, 96, 11)\n",
    "        self.ConvBlock(1, 256, 5)\n",
    "        self.ConvBlock(2, 384, 3)\n",
    "        self.ConvBlock(1, 256, 3)\n",
    "\n",
    "        model.add(Flatten())\n",
    "        self.FCBlock()\n",
    "        self.FCBlock()\n",
    "        model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "        #fname = \"alexnet_weights.h5\"\n",
    "        #model.load_weights(self.WEIGHTS_PATH+fname)\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    def get_batches(self, path, gen=image.ImageDataGenerator(), shuffle=True, batch_size=8, class_mode='categorical'):\n",
    "        return gen.flow_from_directory(path, target_size=(224,224),\n",
    "                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    def ft(self, num):\n",
    "        model = self.model\n",
    "        model.pop()\n",
    "        for layer in model.layers: layer.trainable=False\n",
    "        model.add(Dense(num, activation='softmax'))\n",
    "        self.compile()\n",
    "\n",
    "    def finetune(self, batches):\n",
    "        model = self.model\n",
    "        model.pop()\n",
    "        for layer in model.layers: layer.trainable=False\n",
    "        model.add(Dense(batches.nb_class, activation='softmax'))\n",
    "        self.compile()\n",
    "\n",
    "\n",
    "    def compile(self, lr=0.001):\n",
    "        self.model.compile(optimizer=Adam(lr=lr),\n",
    "                loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def fit_data(self, trn, labels,  val, val_labels,  nb_epoch=1, batch_size=64):\n",
    "        self.model.fit(trn, labels, nb_epoch=nb_epoch,\n",
    "                validation_data=(val, val_labels), batch_size=batch_size)\n",
    "\n",
    "\n",
    "    def fit(self, batches, val_batches, nb_epoch=1):\n",
    "        self.model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=nb_epoch,\n",
    "                validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "\n",
    "    def test(self, path, batch_size=8):\n",
    "        test_batches = self.get_batches(path, shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "        return test_batches, self.model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cnn_utils import *\n",
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alexnet = Alexnet().create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_34 (Lambda)           (None, 3, 227, 227)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_142 (ZeroPadd (None, 5, 229, 227)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_141 (Conv2D)          (None, 96, 219, 217)      58176     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_109 (MaxPoolin (None, 47, 109, 217)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_143 (ZeroPadd (None, 49, 111, 217)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_142 (Conv2D)          (None, 256, 107, 213)     313856    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_110 (MaxPoolin (None, 127, 53, 213)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_144 (ZeroPadd (None, 129, 55, 213)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_143 (Conv2D)          (None, 384, 53, 211)      446208    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_145 (ZeroPadd (None, 386, 55, 211)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_144 (Conv2D)          (None, 384, 53, 209)      1334400   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_111 (MaxPoolin (None, 191, 26, 209)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_146 (ZeroPadd (None, 193, 28, 209)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_145 (Conv2D)          (None, 256, 26, 207)      444928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_112 (MaxPoolin (None, 127, 12, 207)      0         \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 315468)            0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 4096)              1292161024\n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 1,315,636,904\n",
      "Trainable params: 1,315,636,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "alexnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:11: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(array, validation_data=array, verbose=1, steps_per_epoch=2000, epochs=80, validation_steps=800)`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-7765255f4ebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                         verbose=1)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1922\u001b[0;31m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "alexnet.compile(loss='mse',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = alexnet.fit_generator(X_train_orig,\n",
    "                        samples_per_epoch=2000,\n",
    "                        validation_data=X_test_orig,\n",
    "                        nb_val_samples=800,\n",
    "                        nb_epoch=80,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Points\n",
    "\n",
    "Trained the network on ImageNet data, which contained over 15 million annotated images from a total of over 22,000 categories.\n",
    "Used ReLU for the nonlinearity functions (Found to decrease training time as ReLUs are several times faster than the conventional tanh function).    \n",
    "- Used data augmentation techniques that consisted of image translations, horizontal reflections, and patch extractions.\n",
    "- Implemented dropout layers in order to combat the problem of overfitting to the training data.\n",
    "- Trained the model using batch stochastic gradient descent, with specific values for momentum and weight decay.\n",
    "- Trained on two GTX 580 GPUs for five to six days.\n",
    "\n",
    "Why It’s Important?    \n",
    "The neural network developed by Krizhevsky, Sutskever, and Hinton in 2012 was the coming out party for CNNs in the computer vision community. This was the first time a model performed so well on a historically difficult ImageNet dataset. Utilizing techniques that are still used today, such as data augmentation and dropout, this paper really illustrated the benefits of CNNs and backed them up with record breaking performance in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

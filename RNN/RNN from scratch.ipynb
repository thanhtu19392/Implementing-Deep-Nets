{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "This is a pure numpy implementation of word generation using an RNN\n",
    "\n",
    "![alt text](http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png \"Logo Title Text 1\")\n",
    "\n",
    "We're going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we'll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n",
    "\n",
    "## What is a Recurrent Network?\n",
    "\n",
    "Feedforward networks are great for learning a pattern between a set of inputs and outputs.\n",
    "![alt text](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Hamza_Guellue/publication/223079746/figure/fig5/AS:305255788105731@1449790059371/Fig-5-Configuration-of-a-three-layered-feed-forward-neural-network.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "- temperature & location\n",
    "- height & weight\n",
    "- car speed and brand\n",
    "\n",
    "But what if the ordering of the data matters? \n",
    "\n",
    "![alt text](http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2 \"Logo Title Text 1\")\n",
    "\n",
    "Alphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist). \n",
    "\n",
    "Enter recurrent networks\n",
    "\n",
    "We feed the hidden state from the previous time step back into the the network at the next time step.\n",
    "\n",
    "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
    "\n",
    "So instead of the data flow operation happening like this\n",
    "\n",
    "## input -> hidden -> output\n",
    "\n",
    "it happens like this\n",
    "\n",
    "## (input + prev_hidden) -> hidden -> output\n",
    "\n",
    "wait. Why not this?\n",
    "\n",
    "## (input + prev_input) -> hidden -> output\n",
    "\n",
    "Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png \"Logo Title Text 1\")\n",
    "\n",
    "RNN Formula\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png \"Logo Title Text 1\")\n",
    "\n",
    "It basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n",
    "\n",
    "Loss function\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5. \"Logo Title Text 1\")\n",
    "\n",
    "The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n",
    "of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence \n",
    "\n",
    "\n",
    "## Our steps\n",
    "\n",
    "- Initialize weights randomly\n",
    "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
    "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
    "- Measure error (the distance between the previous probability and the target char)\n",
    "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
    "- update all parameters in the direction via gradients that help to minimise the loss\n",
    "- Repeat! Until our loss is small AF\n",
    "\n",
    "## What are some use cases?\n",
    "\n",
    "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
    "- Sequential data generation (music, video, audio, etc.)\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
    "\n",
    "## What's next? \n",
    "\n",
    "1 LSTM Networks\n",
    "2 Bidirectional networks\n",
    "3 recursive networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * encode char into vectors\n",
    "* Define the Recurrent Network\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Define a function to create sentences from the model\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradient and update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137629 chars, 81 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "\n",
    "Neural networks operate on vectors (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
    "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### So First let's calculate the *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 0, '5': 1, 'G': 2, 'L': 3, '-': 4, 'E': 5, 'f': 6, '(': 7, '\"': 8, 'r': 9, 'F': 10, 'l': 11, 's': 12, 'R': 13, 'U': 14, 'a': 15, '*': 16, 'P': 17, 'i': 18, ',': 19, 'x': 20, 'j': 21, '7': 22, 'H': 23, 'J': 24, 'q': 25, 'X': 26, '@': 27, 'W': 28, 'M': 29, 'n': 30, 'O': 31, 'c': 32, 'D': 33, 'S': 34, 'v': 35, '!': 36, '2': 37, '/': 38, ':': 39, 'e': 40, 'C': 41, 'u': 42, '$': 43, 'b': 44, \"'\": 45, 'm': 46, 'Q': 47, 'y': 48, 'Ã': 49, 'K': 50, '0': 51, 'V': 52, ' ': 53, ')': 54, 'T': 55, 't': 56, 'z': 57, '.': 58, 'h': 59, ';': 60, '8': 61, '4': 62, '%': 63, 'g': 64, 'B': 65, 'I': 66, 'A': 67, 'd': 68, '\\n': 69, '§': 70, '?': 71, '9': 72, 'w': 73, '6': 74, '3': 75, 'Y': 76, 'k': 77, 'o': 78, '1': 79, 'p': 80}\n",
      "{0: 'N', 1: '5', 2: 'G', 3: 'L', 4: '-', 5: 'E', 6: 'f', 7: '(', 8: '\"', 9: 'r', 10: 'F', 11: 'l', 12: 's', 13: 'R', 14: 'U', 15: 'a', 16: '*', 17: 'P', 18: 'i', 19: ',', 20: 'x', 21: 'j', 22: '7', 23: 'H', 24: 'J', 25: 'q', 26: 'X', 27: '@', 28: 'W', 29: 'M', 30: 'n', 31: 'O', 32: 'c', 33: 'D', 34: 'S', 35: 'v', 36: '!', 37: '2', 38: '/', 39: ':', 40: 'e', 41: 'C', 42: 'u', 43: '$', 44: 'b', 45: \"'\", 46: 'm', 47: 'Q', 48: 'y', 49: 'Ã', 50: 'K', 51: '0', 52: 'V', 53: ' ', 54: ')', 55: 'T', 56: 't', 57: 'z', 58: '.', 59: 'h', 60: ';', 61: '8', 62: '4', 63: '%', 64: 'g', 65: 'B', 66: 'I', 67: 'A', 68: 'd', 69: '\\n', 70: '§', 71: '?', 72: '9', 73: 'w', 74: '6', 75: '3', 76: 'Y', 77: 'k', 78: 'o', 79: '1', 80: 'p'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print (char_to_ix)\n",
    "print (ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we create 2 dictionary to encode and decode a char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allosw us to create a vector of size 61 instead of 256.  \n",
    "Here and exemple of the char 'a'  \n",
    "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print (vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "The __loss__ is a key concept in all neural networks training. \n",
    "It is a value that describe how good is our model.  \n",
    "The smaller the loss, the better our model is.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the training phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculates the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the training set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculate the backward pass to calculate the gradients \n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function outputs:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
    "\n",
    "xs[t] is the vector that encode the char at position t\n",
    "ps[t] is the probabilities for next char\n",
    "\n",
    "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "```\n",
    "\n",
    "or is dirty pseudo code for each char\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
    "ys = hs*Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but would be time consuming.\n",
    "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "```\n",
    "\n",
    "The loss for one datapoint\n",
    "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
    "\n",
    "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
    "\n",
    "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
    "\n",
    "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
    "\n",
    "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
    "\n",
    "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
    "\n",
    "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
    "\n",
    "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " E2YMzG\n",
      "i08BL4 s11Cf9nDPzfEbUgL*8GK9U\n",
      "NNFSQ2ds,o5)T$\"x*(':lhJc*rweyIesPV(R§yH*OKP$6§R*P(V%*d3JPavuf5r;ÃB'm8X3O5CgKSC-@X6bd.B@-k??FHoI%r8yW;5;G'K9;'ejw1p\n",
      "wPP0m 4xTM@DG2nBo8I5\"9lX'W.r6.MW\"1P,-v$TaGS§c6pf \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "This last part of the code is the main trainning loop:\n",
    "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradiens\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradien technique Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [31, 30, 40, 53, 46, 78, 9, 30, 18, 30, 64, 19, 53, 73, 59, 40, 30, 53, 2, 9, 40, 64, 78, 9, 53]\n",
      "targets [30, 40, 53, 46, 78, 9, 30, 18, 30, 64, 19, 53, 73, 59, 40, 30, 53, 2, 9, 40, 64, 78, 9, 53, 34]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print (\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print (\"targets\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "This is a type of gradient descent strategy\n",
    "\n",
    "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "step size = learning rate\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.861237\n",
      "----\n",
      " GfuHBhAa(oSvkOdLNU\n",
      ",cÃO/lfe@/@FkA'bjfAPX.d;z;p/WF-*Y8!\"P/FJd1ajYmCEl0s0VQpNoIBbRl@xY)H@C§z$2WJ\"5BPd'U2ewOgl)UQAf.fKBB81mg-rbDGCpLSIOV(;h'1w; h\"@m7u.%;n@wTk)\"v%\"'L;9gCA8\"Bz*nLBX:18-PwdYHFMbC(7@F\n",
      "b4icV! \n",
      "----\n",
      "iter 1000, loss: 83.814596\n",
      "----\n",
      "  wrettiait cho hin fge lethn anigs fhero wiscan the ragrl eot do tiungheverm, tracfcse tor cofed the rom aing soDo, he tou theatle pens tiiG the whe tln nfs cireg wpid te int el let tar'ye cab mimrey  \n",
      "----\n",
      "iter 2000, loss: 66.364302\n",
      "----\n",
      " mhas she bucle hed gored hter. acofr omete atelleree ereellute poud this lasteme ooun donk the hed ofe led sh. IDind\"ibec tas wome. Ant uvack altacherte heristt am later Grmete wasteneg. HtenoN Gpreth \n",
      "----\n",
      "iter 3000, loss: 57.942842\n",
      "----\n",
      " ng at int iut un lmucoubk his on he but wavat ong oYihi, caon,, ank an saspsefyerkt wtor carkpering aWw he liog inheasy andcerine sakry taalinnthud; unmy womsaby hither toreresir stsaltyand jrem abluc \n",
      "----\n",
      "iter 4000, loss: 53.826421\n",
      "----\n",
      " purat wot. fouvit Ctam untere hib, the leortay'tter is had shee lortinn; ceang hay with kee afd her kar rHih sti- atd om rate surutry nisoubred;, vein wis faic than ghe math the to one pikig whike the \n",
      "----\n",
      "iter 5000, loss: 56.299571\n",
      "----\n",
      " oupe urepreve bno jon, castowt touE with p7oheecnithe juphing whac; cald7Mn 7st Pe7ing7a wateysiatiln thithing ofte!1 se promsed dime ter ige Phepard Gr7evthe7 a Hack- shot7 7rgothing it en Mim thom o \n",
      "----\n",
      "iter 6000, loss: 57.119782\n",
      "----\n",
      " einy ean hat hersy, Ufather the condly womt rol teanere cos anne whed but then; himseded date lalltiptcas Iqxitceith nouled siss don doug bice alded mald thine lgoute want to Ngtertt sortadact thimtio \n",
      "----\n",
      "iter 7000, loss: 53.030488\n",
      "----\n",
      " ern.\n",
      "\n",
      "Uof for in as ow arsedwowige goom wous ul iveg hit roing.d anPboing dateccha for, coracl, be clithoutt mlap of exam'that was le.w. crenshed athijus wist, parceadl gans\n",
      "ailins shis saring ha mece \n",
      "----\n",
      "iter 8000, loss: 50.275129\n",
      "----\n",
      " omerted It abd than sisf platten waass thithaterrtay almen the womiat whe stom to ha sestere momecasrere would, led bletaned the to foof ie and and of\n",
      "Gurlice, waur hises orely woultlf, be were at upe \n",
      "----\n",
      "iter 9000, loss: 48.984878\n",
      "----\n",
      " d becoungatp undon douteningladr the arund wtatper room, his ounted had maras in ihern.\n",
      "Seduirener tort hould urden, as hincry her's moubl? simerering rofthorh.\n",
      "\"Jome bred would mide wong mins anien,  \n",
      "----\n",
      "iter 10000, loss: 48.599734\n",
      "----\n",
      "  sithat, he wis datpalk conbllea ind.\"\n",
      "Gr'st loom dongoren inwns o he to brenst letse his sid sof that nat caratemy would binining thot and ally. She leanifle he pulle. Thes shrlinin fhige pary thecl  \n",
      "----\n",
      "iter 11000, loss: 55.021080\n",
      "----\n",
      " citecurdate farzeqferm\"dar fof of th0\n",
      " upiit usiceidenng Mth Greglalpct ofere comenctf andall sowangm.\n",
      "\" It tudass Gre8mike lhik;. sicailt-pies ditortechte\n",
      "\n",
      "Thattoy her any fratione\n",
      "\"-thens furded on. \n",
      "----\n",
      "iter 12000, loss: 52.046876\n",
      "----\n",
      " have ghe wastenged was furld tofy ugelade the asl, a foreng. Wned apeclmsechthore had all, wad for of that so asle he cos nouge ager to the ppleevegill, the was bubnigley Ir hall, wiels non listttare  \n",
      "----\n",
      "iter 13000, loss: 49.117559\n",
      "----\n",
      " ubsifoy -ver rugor; to'n her hore, himad wNould stry, his it; hamnis in though the boDly the elfped shas bat evemtent, betey llpas suld opfall miltyingole havald and - to was ethere not nowhiugh dild  \n",
      "----\n",
      "iter 14000, loss: 47.501631\n",
      "----\n",
      " at unteth starat. Ir be so that had stengeo thes thaer way had faigrhele llaned Gregor;., the man pace siasstroove sid, wist the ally stele unhit\" untwaucict to hert a dle efloit !\"1 Gregor's kGiild m \n",
      "----\n",
      "iter 15000, loss: 46.933157\n",
      "----\n",
      " ed. for perecrcend more mishorady ans silled dithed I flemimabe, domt Gred and bedyst ontrel thout ald then. Shagh lfoune frether alrow farce pars him the doocist, to it.\"\n",
      "ving wate ould in hime the'; \n",
      "----\n",
      "iter 16000, loss: 49.833302\n",
      "----\n",
      " o Un the with ths oR coke come\n",
      "% tis comed coced. \"GOf. \"Lerain the beteseenenit. TT\"6**/LT)2TTHELESF UACQugo! Susprour asadery\n",
      "\n",
      "Sad, the dothe\n",
      "\n",
      "The mold.\n",
      "Sed with worctned st allns erwiin Gnensas coc \n",
      "----\n",
      "iter 17000, loss: 51.784741\n",
      "----\n",
      " y neass inteton, to the qous bust and the forn fo tho crout they her lark want. He lade to wort boks dist Gregsrm ass eo's sakn shage that forbuga; not betarly ates\n",
      "ate bed if by wald thaal ovect it h \n",
      "----\n",
      "iter 18000, loss: 49.023205\n",
      "----\n",
      " , herto nat, dasf thouck he werecf the datmereny to ondor's he the mowtthat thale alad ouf nos dibe obro this undion wosk alfer had cloul! to to s pas h, daiked beghating abring his the eald. He that  \n",
      "----\n",
      "iter 19000, loss: 46.970997\n",
      "----\n",
      "  whaw shan aill his begoke- beatarvey. His very oIt rot wore and that. But wis and ate any upyon, pabte had or opyen:, the qoot perses has hevenf tidl anly favct of sa rough, \"ight she love and for ur \n",
      "----\n",
      "iter 20000, loss: 46.156152\n",
      "----\n",
      " ber, aver was uw sen he chest, botgely any, fro's furred as towor thainony spremriof buth, yo  as rair - mlaner erwas frently zitk. Sued in the to thea of then so hick hed ware, himseord, steont hod t \n",
      "----\n",
      "iter 21000, loss: 46.063529\n",
      "----\n",
      " om stay deras thee on\"sing list they for juch teem nemay espaim bereding wamy fus and tiwert what Gregore had biom at cacling the chime and wopsolo lichoute hit lith wourd. \"prented that. Mr'ghtt it h \n",
      "----\n",
      "iter 22000, loss: 51.671821\n",
      "----\n",
      " vemsiont mid tifunidirtull diteme vith a do lenaccern Yregor and ing Mrmeant of andareiacl: Whe witheb\n",
      "GLe\n",
      "tad ictittat and and, waviss yink restith in to essene the becank worodily tonatinlise mpersi \n",
      "----\n",
      "iter 23000, loss: 49.390650\n",
      "----\n",
      " erlatrous fered the sterem, the dont lipund overbed, the  do spich gat erone he hidsila arsed bing steaspment awa roje nonk thare; thith had tioug. Who:, the ting thould whist fersilug as than, he was \n",
      "----\n",
      "iter 24000, loss: 46.939800\n",
      "----\n",
      "  hat he s suxtiog sus feeck muis werte toating  hey of eay a nect droved coom had thernings cood as hay. for ferpe shis beinint the\n",
      "ter thas in the perene\" pinct weme he shop stare tnoce get was had e \n",
      "----\n",
      "iter 25000, loss: 45.481420\n",
      "----\n",
      " hel eveas seore and lo camed silked of Gregol't as the. Bo theurethef cout aifler.\n",
      "Stlepe h anly of turter; stenem the rantoly in The formayded theyse waed.\n",
      "\n",
      "Pouthor't his had whine shit of his floce  \n",
      "----\n",
      "iter 26000, loss: 45.112311\n",
      "----\n",
      " w, the asme the gack would by anly foren if the sgots alo and not he wist. Tout upmeald seirlk, if whimeack the withed theucegores h medpentent uncooked, hally und all ens hid to Gregh whild ves thes  \n",
      "----\n",
      "iter 27000, loss: 47.600626\n",
      "----\n",
      " on'd the\n",
      "cherfliciinath-re orey got - lomerthorn, of, ally he him, moss of to nernathougays. Oe'm't tating preaning and as\n",
      "senteng whiegioul ated tomer,\n",
      "dais, wine as be his dather\n",
      "\n",
      "sow formed le with \n",
      "----\n",
      "iter 28000, loss: 49.490699\n",
      "----\n",
      "  \"Copding in con thele tart acmemporcaite h utint tfen a solusen shies the clave the crave, hamsint if ellcangr. Dimenbeem could eachiit ly bung how theS hudion. He geve. Sore bod ig the fourbullod as \n",
      "----\n",
      "iter 29000, loss: 47.109378\n",
      "----\n",
      "  moot's shathitt foute Daon horegor oner tsen hick, at lio, the dapl lamew as rongt comed ins of whis nacooution guine if and tussidicl hay hom internoor inses that as wismone to gomtindins the insgel \n",
      "----\n",
      "iter 30000, loss: 45.335735\n",
      "----\n",
      " gowe. \"ICh Fouthent, the wa could ther anaor as had t- hoon starhed not leald coucite. In's hapeing and lfith liged mars to he monk, mome, betarleing the ind the cas and and formout ary tulk he beite  \n",
      "----\n",
      "iter 31000, loss: 44.697868\n",
      "----\n",
      " icners\"ar, antore stesthod of de enet it weed stome; spranigugher stome dia nitinger of worent a feank. The of that her ate trown, foy and ooferenn to s Litt they. On the fared to bearise wheren hon c \n",
      "----\n",
      "iter 32000, loss: 44.579990\n",
      "----\n",
      " flod, them viin\"\n",
      " ro\n",
      "\n",
      "wnok od, legt floc and, dimsitey. \"fole deacilins, ank, sthe jusnigichathings\" her thise by, and bnsanding the atio's lowich at thaw wslo soon fle cor. His hersont, the wirter,.  \n",
      "----\n",
      "iter 33000, loss: 49.550673\n",
      "----\n",
      " nl ancart\n",
      "Germerasine) p micecr at you arr. Bute of notut site tidiousluld cariistlould asrnoonselint otherssoons tfoup uxtiomsenonlib beayethel: But than itp lect in tire\n",
      "tokst.\n",
      "\n",
      "He wordellies withro \n",
      "----\n",
      "iter 34000, loss: 47.702828\n",
      "----\n",
      " ing the could heet in trought orme foom that dome thiikely, this morcair himsallent sar ouchase recter. gor. Greqoigh comed ould mack a rous wutce, alr, them\", Gregon't ock, could undaring. Shey ingay \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 45.489661\n",
      "----\n",
      " ther not but simse, which to alpmere.\"' arridlde the lull the sasren havins ly the aith had onte sewer wullugh't coudly rich Gregor's ned wout leveng carded bue could work becaps troidsed. That prof a \n",
      "----\n",
      "iter 36000, loss: 44.147101\n",
      "----\n",
      "  bechu not that mowl relelys ast, he reast terinly the moar. jut, werkixy to the coulln had sas ensted fattossed fron in roaghing, Gremores nos and Gregor it kum. Bow himstlicr that awrot would him.\". \n",
      "----\n",
      "iter 37000, loss: 43.913077\n",
      "----\n",
      "  He afsiiged the mossmed, le almfor miss. Or., etiof. Whiong to and with it wable for's sow ithinf sawing foull of sicely would bead horesly; contit; kawd ick in there has plera life, even liformithe  \n",
      "----\n",
      "iter 38000, loss: 46.039288\n",
      "----\n",
      " n in adonbed pasced, as hid Projechid this clihiber and lime, tiles famacaghtron't wate coply or\n",
      "Mr on mectow wmlerandand. The dire wickegired prowntengf he firm evere out for in chatey?\" Urate\n",
      "pprion \n",
      "----\n",
      "iter 39000, loss: 48.028622\n",
      "----\n",
      " spare rom to ablarinesd wher lulpy ho howith.\n",
      "\n",
      "At caviare longst at he mothem butmealeyt s insallat of go reight s Gregor the leeve abusise he he mule condys alleniey to se, flons movearly his buting  \n",
      "----\n",
      "iter 40000, loss: 45.844110\n",
      "----\n",
      " ect cewed werwe carssedsunnd aalf thit woukd, and frenthat Saich if that for wory nherely tfiek. Aven he was that wos ofresting to shen whath.\n",
      "\n",
      "turniat pade.\n",
      " \"wad timed the. Ald lesthully womemtany t \n",
      "----\n",
      "iter 41000, loss: 44.209802\n",
      "----\n",
      "  by mowne all\n",
      "d whist to more plomene habded at ho hamme it by momp. Gut offuny war bouge was what more the nonill you plest some arsegirp when his it was bewestass ating no fore, quy leched, couce th \n",
      "----\n",
      "iter 42000, loss: 43.700378\n",
      "----\n",
      " nstas warwarlyt, the droted ver his ees leaking. Whating dascroy had way hid coverbay were thangote time toy betail tor, of tment prousmomtasing umegole: \"And whalfing fake ard, beseing ense mather tn \n",
      "----\n",
      "iter 43000, loss: 43.616937\n",
      "----\n",
      " ess his so mispowilvs was it himiNg a maght so pas voited nist it sure sict raise puspyitting; for flowed and whaw shough forded and ay had to elscer the simer her whookly to when so him it was with h \n",
      "----\n",
      "iter 44000, loss: 48.124514\n",
      "----\n",
      " erbecurslat2teact\n",
      "Gren of hep ar appeomm wititat Mrsatre of try us he Projecoutly you, afyed le\n",
      "pore lections.  Tfelpory ary and elotiave fetle to sanintripigainised betorer) Lich.  The easim.\n",
      "\n",
      "Fove o \n",
      "----\n",
      "iter 45000, loss: 46.558757\n",
      "----\n",
      " ng arlaid.  Cad luistcad. Pless Gregor whith, al gege, by in anwessipe cinxelt o her. Wel ondistros; he Rean werktroke his chang?\"\n",
      "Swager. At. \"This be. An his cetion, if my wave had sfood Projech dar \n",
      "----\n",
      "iter 46000, loss: 44.504226\n",
      "----\n",
      " out she eand somedely ching all, thent. Gregom the peralure stit, do%e cuiminn, and the worly she ald himmediress - rute then in bomuned eveadared breghe to lerastatiof the Prougstayl makoute the havi \n",
      "----\n",
      "iter 47000, loss: 43.231797\n",
      "----\n",
      " er esonet aned attiup. Is hafe and cevearly no carking becongoon themy could his poom the preleth expeinw stayhick siscrrat sirterorp, carthor sould cotsente. The dos, any on emeatindlust was had ay,  \n",
      "----\n",
      "iter 48000, loss: 43.042424\n",
      "----\n",
      " tothing, fom bo that's his\n",
      "room soon; nos uping to anden the dath his lathing fut. Fon to sew fut mo wassed bun bute- and sulosed not ane inly. Sad and a was in raom file roos uckus so and to with hap \n",
      "----\n",
      "iter 49000, loss: 44.972668\n",
      "----\n",
      " \"s Sher on you lefcenXt; a solvidion then clan yry. ben' wit insed Gregone.\n",
      "\n",
      "Stuntoremed\n",
      "\n",
      "on charre inse, wibkg-ins fasect Gut rain or whit ibppet leangor and the could in whin) lowise meiblean doog t \n",
      "----\n",
      "iter 50000, loss: 46.987170\n",
      "----\n",
      " qofed woldon for 1.Y.1. :G Lhes araich his encnely, dle he call intoupmend celiresfoon It madesd.d anl that cafes laxcen wesm as to thach the's etould it you plapsuge, dore to thauli. Whagh theed that \n",
      "----\n",
      "iter 51000, loss: 44.974396\n",
      "----\n",
      " y the coscely difh the hurrior gene, shat not as only room pean wo leafthrimunt sed to hindong ne as that whispenfly the fas aplught himcay the at her mowr to Gred. But that where shis sur. Irees of i \n",
      "----\n",
      "iter 52000, loss: 43.430705\n",
      "----\n",
      " ere hat An go con th here to shet with his father and againg a hirsesvict on him slowh suren, and, suren wiren the rellaght; serens pomiccrugit was iht and thould and doont ally of melely the reise as \n",
      "----\n",
      "iter 53000, loss: 42.971043\n",
      "----\n",
      " evely where hes boostp agreemped not enongrion tome to the wailess. Would not staull the fom to his out stoped to whered, of the door werkss in theee bee lay bechem; not mas of yound out begolk for th \n",
      "----\n",
      "iter 54000, loss: 42.891972\n",
      "----\n",
      " he kiat heakg to Gregor'st the rerlay condwallit; his father. For, where. De? coutelais tereathor had even't endoss he habked his was had spovinteng you dlof to tureacirng dee pleated grat a fint igti \n",
      "----\n",
      "iter 55000, loss: 46.949092\n",
      "----\n",
      " .\n",
      "\n",
      "\n",
      "Projy streores -in eate of ofres to restingions oww arst and partaim.  Cow ont ondidation; room\n",
      " tiete tightusionraremer opresupt the more to reate simtoor antayliseyadeden\", Projectud,\n",
      "3uter alyn \n",
      "----\n",
      "iter 56000, loss: 45.714285\n",
      "----\n",
      " t wisppect the \"comend, thought Gregosgone gourly to to resirus to heq caised plefas thation on Gregher caod of aple gistersing in do Lim; to heve and out elect, lems by alsse be?\"'d knocd\n",
      "be of her s \n",
      "----\n",
      "iter 57000, loss: 43.751312\n",
      "----\n",
      "  lint from his ost Gregore, donniadith and anf she tam the butrastrowe puty wely breas acad dove upened O( - migrsion the chouch do regoment thaed couseabed,. \"OR's dispentiford talced dome soth amsed \n",
      "----\n",
      "iter 58000, loss: 42.572871\n",
      "----\n",
      " f the dathe'm was avay to pref, discer lo would able there she whosted en\"tore, withem her bed that him expemst thay uppor by. Seemeatso wayg at to his feast wind of but as wosture she wall had sartin \n",
      "----\n",
      "iter 59000, loss: 42.383022\n",
      "----\n",
      " s juik for theale Fond bust neave whe caiculling; repanitser mack it duid, himpsoly at bomisatt. Prout anfring bigrm, whained to mefinn is; lister as of clomine fort slong the fite. Whr to the had wou \n",
      "----\n",
      "iter 60000, loss: 44.178841\n",
      "----\n",
      " that yourenor to igack has she wuthers but mare, toully in weble sher make haage. \"The exieecong thane in near an hade not like and.\n",
      "\n",
      "**e regted his frest Gregeribed, prey the vering memsely And of at \n",
      "----\n",
      "iter 61000, loss: 46.153700\n",
      "----\n",
      "  shourire him hadre the clical lonergong erovidely and where notiruplghat\n",
      "and beve stowry arming bastiet cisclysmabut tus workes: \"or all he cadmes so his mead hitally anling. he capaciingont aicly bu \n",
      "----\n",
      "iter 62000, loss: 44.275381\n",
      "----\n",
      "  well seidef upron the sedurnt; an reoved to had a feess you ogce her lef enone, and on he meaving a and chilt, had tcihepp! he was chesm yee exully, to dakeftules of him;'t ondcribly high in to could \n",
      "----\n",
      "iter 63000, loss: 42.799501\n",
      "----\n",
      " gh ill will lefying, to ever and her hable the to rerorsing and his chite fadry or a sact, githurion soleb any to to him tiegor Phe what the brigal lixt simaing ar time to sid his an siding. The becan \n",
      "----\n",
      "iter 64000, loss: 42.342507\n",
      "----\n",
      " ont, the roveresping earghis.\n",
      "\n",
      "He Proure all muthly was withrtrallad and teraincis towe wable ever for undiin, mowns, he camprore not whicy, brentedly deances thes casyy that and he heldy viod has cam \n",
      "----\n",
      "iter 65000, loss: 42.287645\n",
      "----\n",
      " gone had noped a butlect and a orssing she him, ag in would she koatsoneds his door and sursed thet of the his nother any butiouing deast femanist even to parthen, no wairicly ut. Ugchr companas eveaf \n",
      "----\n",
      "iter 66000, loss: 45.958894\n",
      "----\n",
      " n, thes terse Fit on binsanence sea wisis the quorthapigettion fortoutairs liftlion to undere Flo impidit. unwoy that Literlilf he wite is rot Greg-tm\n",
      "pisperliciting with thead Greation. He keatrotmon \n",
      "----\n",
      "iter 67000, loss: 44.994836\n",
      "----\n",
      " wiuly uppleaply to of but as and whating sodeanlly sulitul the wlind, stow wourns thing oficasten back withlurent that mevey even hourning his clerianly.nHt thoully sat would kelpent the welk reims it \n",
      "----\n",
      "iter 68000, loss: 43.132866\n",
      "----\n",
      " ng was alribughing butint, pyaeling - could comany undous now off hads simpes, br. Ies I cony speticem this diveled boontly Gregor\n",
      ", did apire, though him. Pregold tow wie's sigone or had just agailet \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 42.019014\n",
      "----\n",
      " ickughed with legonly hownw us it now, breever.\n",
      "\n",
      "The sistarayou here frished apmeare do her Gregor; pilitured seint from hey, to turepseand that swessand neally lita mode, alther and werte there, at s \n",
      "----\n",
      "iter 70000, loss: 41.802334\n",
      "----\n",
      "  theate myited thomgeling ally thouaves tome pillught tho galtenien, on lay stening wa anilpeding, cardiate methed back they serlingo the khaw.\" eBohed to it zother thould spoad could, and inte the on \n",
      "----\n",
      "iter 71000, loss: 43.545693\n",
      "----\n",
      " estits of that expeadiony wis wain the ssereays even over tl/en, caru been ary bome is work wout Greg' toid up bus agen the flowy to futieverciat tre ingoingernse in das with the d Gregor is. OE F0I - \n",
      "----\n",
      "iter 72000, loss: 45.443278\n",
      "----\n",
      " es, staps con'lared Gregor. Ham, where samwed efoom or in tratietulen,, fire nfons his Unw woulding they meen weten he no noed betmray reall rorgings it aveings, ploheming. Plescely uis -t: I'm, bralk \n",
      "----\n",
      "iter 73000, loss: 43.663044\n",
      "----\n",
      " satternown?  no Gregor iathether, 5ure reais andysultse reandendly room harded from the was strtomaid of had semmait had wried net. Ses his last comtreme dis fransire ally fastlight but his was opentr \n",
      "----\n",
      "iter 74000, loss: 42.254775\n",
      "----\n",
      " ? nos simied scomily, was that sitter himself and coused no back agiled woas beave had and Gregor's, now han carnew of haichuter firsiln alw the caladary thet wore uis lack it. gen asly aly homeed upl \n",
      "----\n",
      "iter 75000, loss: 41.863719\n",
      "----\n",
      " d, be ham awly they larged her iever out, con'cer forrerbothen out the lain for his of quott alm. Then stopstes to leaneny strened\n",
      "for ondse to mofing his had is not he was bute his cwork the of entin \n",
      "----\n",
      "iter 76000, loss: 41.777145\n",
      "----\n",
      "  holy his tid all runing antarsed in durr wos sideed and rout have her. If there it, babliin mefleraasad. Gregole. Sas deap don the lecting as the wand mas held way with rusiir copeneedpup last in tre \n",
      "----\n",
      "iter 77000, loss: 45.164248\n",
      "----\n",
      " could wesmenh in pinded of thes cearly she prairy armfoom\n",
      "and anmerendatad contatied thouge any orsenting GrePle flosiculy trmain in ticactroutong; the sentions on medsionct withercite samiin regicing \n",
      "----\n",
      "iter 78000, loss: 44.444349\n",
      "----\n",
      " eded as to wos therd it bust, dus? He coully, any thime even of the Projery distaully. They could sart wast roy now hade, iny bromsion where not arpengect of and had gexk of vief what sive as ald hour \n",
      "----\n",
      "iter 79000, loss: 42.631042\n",
      "----\n",
      " it efont they pake, had herd sitsly do nit nit sealcing? Now?\"gor to to dist I to had of by it if thoughtnotpy my up ontharet\n",
      "anly brobeding the dimeall\n",
      "vere! With him. Bum. just and his lone, brerest \n",
      "----\n",
      "iter 80000, loss: 41.561377\n",
      "----\n",
      " hiss, to sore tibuis lart workes to comed when if stere welningeastlly alrigutions of was fateng that to hanflods and thersus latghe wioddy. \"foÃ whetes, Grojult over all sw. orever nonture, but senpl \n",
      "----\n",
      "iter 81000, loss: 41.366847\n",
      "----\n",
      " oot sow efored notoup his towd to wink tare turs tore'\n",
      " to- if simpely and have at now waulen a quired be plait to her riginert dakeayr at same had to and bire tere just that and worke bow, in; whowin \n",
      "----\n",
      "iter 82000, loss: 42.958823\n",
      "----\n",
      " if now! the could the work wis wereat and the finseld\n",
      "bod, lovyiinstly it. He would ongeed comasion. Thanflhesing Gregor. in tamwiped\n",
      "bond any Gregerycede usis to Listiist shemo Gred unserkine thes an \n",
      "----\n",
      "iter 83000, loss: 44.852643\n",
      "----\n",
      " imson the  past atmenfell\n",
      ", sichith ale by be andaided abestes. You was terked.  Wow upont alot wonk on of the veat and and wout breaint up prromed ares door but he mas coter, even, himsla foot lo and \n",
      "----\n",
      "iter 84000, loss: 43.228890\n",
      "----\n",
      " re. What he lave to her olly, oven lobcuse with oherer the laichtait he had nack ind of the seal the daich, dus was fading stow, is froving when'verinly, waumad@rs, takcied, a fate ofle site he to kur \n",
      "----\n",
      "iter 85000, loss: 41.807966\n",
      "----\n",
      "  - rise been moffeved un the door and be thagh had take he ensnent that jeempys holire the jurs, the kiont; you hey inforther his wain haw? nlisuny for that offor tuntrathinly, up now sirast parzy.\" 3 \n",
      "----\n",
      "iter 86000, loss: 41.488058\n",
      "----\n",
      " f day his sfoon, thesint; he coulded at be along on Itoe\n",
      "way he was bowaved as her bing reskes I distwh of the louginly, relled into exkenllr lyocice move the pach kight to shes whe ragh the father aw \n",
      "----\n",
      "iter 87000, loss: 41.365950\n",
      "----\n",
      " n ase msessurt shit mading she dry his stat themingo yut him from jute extort in the dimed she heven as would Is mpen up eaking all pyound his father his moon letely, havesboem did puse agarom the rou \n",
      "----\n",
      "iter 88000, loss: 45.132475\n",
      "----\n",
      " ive to edeate\"d o\" copy coden rother, the Project\n",
      "Gutenbing a losce inthel at\n",
      " autirigem noten o's\n",
      "chainbect difase ancouby wound tillsing formatiavin Mregor on the Proj\n",
      "ow\n",
      "test face wond, price the P \n",
      "----\n",
      "iter 89000, loss: 44.322893\n",
      "----\n",
      " y in rean; now your whelutter the fifa, tw anlay comsincching make but of its\" sure, withovat wund was al Gregor ansay, sive stent and Grojy works heach hin woulliked, thet Fistence that hard beem han \n",
      "----\n",
      "iter 90000, loss: 42.329336\n",
      "----\n",
      " t in then frohided mornted solly sort sternging.\n",
      "\n",
      "\"veacks whing\", engelly seafor could as is the fenant. Ard seesined Gregor gaver. As pouthindly no defich tion apany a cabiredead sentelidg whiin paid \n",
      "----\n",
      "iter 91000, loss: 41.297128\n",
      "----\n",
      " xpompenvy gake of takent had sester that bacook. Ham room; and buch, h peat horh it. Gregore buste ne sooning of alrintied of the fare tulk to, se spoors the orcound seent any onese. Hee to thot was t \n",
      "----\n",
      "iter 92000, loss: 41.019441\n",
      "----\n",
      " ar the tointerunt that he caneis whididly of then had heal in where aimisont.\n",
      "\n",
      "Gregor cemiven-ying to hee door have ity onthasy the other\n",
      "puathas were, thein onten had pretire on the time finter and s \n",
      "----\n",
      "iter 93000, loss: 42.513311\n",
      "----\n",
      " ert\" in for any jegecmed. Gutd, make and he wond thaked of thes any but with pre-te slomalily asneoch of her to the a purely sarint day on man her wablt with with with she reghtm torang un kould to th \n",
      "----\n",
      "iter 94000, loss: 44.466585\n",
      "----\n",
      " out not himsion about in the woweang one ty a not that lifted himsion of anlith.n have by the held bide clourningort becrall pare he ut than was to bees tine anself he that of may him. Gregone is the  \n",
      "----\n",
      "iter 95000, loss: 42.884919\n",
      "----\n",
      " rdabledtmailer, wherry onat for the father dish nan - dopor; that Gregoright and hald round of the head, yinded one she every, Mrmarest, not for him bleduse the flet his cleffee his fitter prarkes hel \n",
      "----\n",
      "iter 96000, loss: 41.467508\n",
      "----\n",
      "  it the lepe hed beee, on to ceant sairing pole only, That of anly quimed his sisst now silpen the elight in there wipleough beemesus not out to ex\n",
      "adlow, jume with a waster dows stare, than him, sect \n",
      "----\n",
      "iter 97000, loss: 41.201556\n",
      "----\n",
      " ollstablit of the tor, by to hive's not, Grenotsly the other - straid with where, more and shaund of bfit effies abengord, he wan the sealpmaile\n",
      "fald, then thu, actor. You canded, and that ther Iim he \n",
      "----\n",
      "iter 98000, loss: 41.087501\n",
      "----\n",
      "  and into herthernghled fation himne pare that have would workt, and enstore and itssay whem of hel; he jatetcout it equer fis lieted sin ine orke the way the fancunerpuster tote then'ck they he wast  \n",
      "----\n",
      "iter 99000, loss: 44.026971\n",
      "----\n",
      " ey ow yse insly up be expenkenvigurn ansefatanitend;  Greghtmoruad to quspure. GreWin) easing frece\n",
      "the so to themer, orss, pomy at compiromide Gutdens, a quairy an shis orveand to $E TI D-, then than \n",
      "----\n",
      "iter 100000, loss: 43.548277\n",
      "----\n",
      " ng feece bealln't gfelly intelly imso him an, not facrngate beentt woulr his migred as he medealy thingsald ment fortam to the door wert and with he -erobly he wagh and \"I'd the coudln. Itt'th in srin \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is credited for [this github](https://github.com/llSourcell/recurrent_neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
